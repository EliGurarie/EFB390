---
title: ".white[Sample Counts: Quantifying Uncertainty]"
subtitle: ".white[EFB 390: Wildlife Ecology and Management]"
author: ".white[Dr. Elie Gurarie]"
date: ".white[September 20, 2022]"
output: 
  xaringan::moon_reader:
    css: [default, default-fonts, mycss.css]
    nature:
      highlightStyle: github
      countIncrementalSlides: false
      highlightLines: true
      titleSlideClass: ["top","center"]
editor_options: 
  chunk_output_type: console
---

<!-- https://bookdown.org/yihui/rmarkdown/xaringan-format.html -->
```{r, echo = FALSE, eval = FALSE}
renderthis::to_pdf("Lecture06a_CountingAnimals_Uncertainty.Rmd")
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, las = 1)
#output: html_document
```


```{r colsFunction, eval = FALSE}
system("cp ../mycss.css ./")
#system("cp images/aerialcount.jpg ./bg.jpg")
xaringan::inf_mr()
```
## The trickiest thing in sampling

Is computing the *precision* (standard errors / confidence intervals)

![](images/MillerEstimator.png)

---
### General principle: The bigger the sample, the smaller the error. 

.pull-left[

$k$ sample flames with counts $n_i$, each of area $a$ out of large area $A$

total area sampled is much less than total Area:  $$a_s = \sum_{i=1}^k  a_i = k \times a \ll A$$

]

.pull-right[
then: 

$$\widehat{N} = {A \over a_s} \sum c_i = {A \over k \times a} \sum c_i $$

$$SE(\widehat{N}) =  {A \over a_s} \sqrt{\sum n_i} = {A \over a} {\sqrt{\sum n_i} \over k}$$
]

--

.pull-left-40[![](images/Pop2.png)]
.pull-right-60.small[
.darkred[

- $\widehat{N} = {100 \times 100 \over 10 \times  10 \times 10} \times 21 = 210$

- $SE(\widehat{N}) = {100 \times 100 \over 10 \times 10 \times 10}  \sqrt{21} = 45.8$

- $95\% \,\, C.I. = \widehat{N} \pm 1.96 \times SE(\widehat{N}) = \,\, ...$

- Coefficient of Variation = ${SE(\widehat{N}) \over \widehat{N}} = \,\, ...$

]]




---
class: small

## Example - single transect, simple formula

.center[
$\large SE(\widehat{D}) = {1 \over a}\sqrt{\sum n_i}\,\,\,$ and $\large SE(\widehat{N}) = A \times SE(\widehat{D})$
]


```{r}
source("functions.R")
set.seed(9)
Z <- setupPop(100)
```

.pull-left[

.red.large.center[
$n = 8$; $a = 1000$; $A = 10,000$
]

```{r, fig.width = 5, fig.height = 5, echo = FALSE, dpi = 100}
performSim <- function(N = 100, width = 5){
    Z <- setupPop(N)
    sum(Im(Z) > (50 - width) & Im(Z) < (50 + width))
}

reps <- 1e3

n.sims <- sapply(1:reps, function(x) performSim()) 
par(mar = c(3,2,2,1), tck =0.01, bty = "l", cex.axis = 1.2, cex.main = 1.5)

plotPopulation(Z)
segments(0,50, 100,50, lwd = 2, col = "darkred")
rect(0,45,100,55, col = rgb(0,.5,0,.2), bor = NA)
```


]

--


.pull-right[



#### point estimates

$$\widehat{d} = 8/1,000 = .008$$
$$\widehat{N} = \widehat{d} \times A = 80$$

#### standard errors:

$$SE(\widehat{D}) = {\sqrt{8} \over 1000}  = 0.0028$$
$$SE(\widehat{N}) = 0.0028 \times 10,000 = 28.28$$
#### final abundance estimate:

.darkred[
$$\widehat{N} = 80$$
$$95\%\, CI(\widehat{N}) = \widehat{N} \pm {1.96 \times SE(\widehat{N})} = \{24.5, 135\}$$
]
]


---

.pull-left[

#### This is why you *want* lots of transects:


```{r, fig.width = 5, fig.height = 5, echo = FALSE, dpi = 100}

width = 5
    
Z2 <- setupPop(100, updown = 2)
y.transect <- 20

par(mar = c(3,2,2,1), tck =0.01, bty = "l", cex.axis = 1.2, cex.main = 1.5)

plotPopulation(Z2)
segments(0,y.transect, 100,y.transect, lwd = 2, col = "darkred")
rect(0,y.transect - width,100, y.transect + width, col = rgb(0,.5,0,.2), bor = NA)
title(paste("n = ", sum(Im(Z2) > (y.transect - width) & Im(Z2) < (y.transect + width))))
```
To capture variation!

]


--

.pull-right[

#### This is also why you go along the **gradient** of variation:

```{r, fig.width = 5, fig.height = 5, echo = FALSE, dpi = 100}

width = 5
    
x.transect <- 20

par(mar = c(3,2,2,1), tck =0.01, bty = "l", cex.axis = 1.2, cex.main = 1.5)

plotPopulation(Z2)
segments(x.transect, 0, x.transect, 100, lwd = 2, col = "darkred")
rect(x.transect - width,0, x.transect + width, 100, col = rgb(0,.5,0,.2), bor = NA)
title(paste("n = ", sum(Re(Z2) > (x.transect - width) & Re(Z2) < (x.transect + width))))
```

.red[**gradient**] - means slope of (steepest) change
]





---

## More complex formulae


 from Fryxell book Chapter 12:

![](images/fryxellformulae.png)
These are used when **sampling areas** are unequal, and account for differences when sampling **with replacement** or **without replacement**. 


---

## Simple-SWR

- **Simple:** Equal sized sampling frames $a_i$ all equal
- **SWR:** Sampling 'with replacement', i.e. frames *OVERLAP*; some individuals counted more than once.. 


$$SE(\widehat{D}) = {1 \over a_i \sqrt{k(k-1)}} \times \sqrt{\sum n_i^2 - {\left(\sum n_i \right)^2 / k}} $$

$$SE(\widehat{N}) = A \times SE(\widehat{D})$$

variable | meaning | in book
:---:|:---:|:---:
$k$ | number of units sampled | .green[*n*]
$a_i$ | the area of a *single* unit | .green[*a*]
$n_i$ | an individual sample count  |.green[*y*]
$A$ | total study area |


---

## Example:

.pull-left[
```{r}
source("functions.R")
set.seed(2)
Z3 <- setupPop(100)
plotPopulation(Z3)
sim <- countSamples(Z3, k = 4)
plotSampling(sim)
```


.red[
**data:** counts = {2,3,1,1}

a = 100; A = 10,000]

]

--

.pull-right[
- $\widehat{N} = {2+3+1+1 \over 100} \times 10,000  = 70$

- $SE(\widehat{N}) = {10,000 \over 100 \times \sqrt{4 \times 3}} \times \\\sqrt{(1 + 1 + 9 + 4) - {(1+1+3+2)^2\over 4}}$

- $SE(\widehat{N}) = {50 \over \sqrt{3}} \times \sqrt{15 - {49 \over 4}} = 48$

- $\widehat{N} = 70; \,\, 95\% \textrm{CI} = (-23, 164)$

.green[*Anything wrong with this confidence interval?*]

]


---

## Example:  More Heterogeneity

.pull-left[


.darkred[
**data:** 

- counts = {0,0,0,8}
- a = 100; A = 10,000

]

```{r}
source("functions.R")
set.seed(5)
Z4 <- setupPop(100, updown = 10, leftright = 10)
plotPopulation(Z4)
sim <- countSamples(Z4, k = 4)
plotSampling(sim)
```


]

--

.pull-right[
.green[
$\widehat{N} = 80$
]
$SE(\widehat{N}) =\\ {50 \over \sqrt{3}} \times \sqrt{{(0+0+0+8)^2 - {(0+0+0+8^2) \over 4}}} = \\ = {50 \over \sqrt{3}} \times \sqrt{48}  = {50 \over \sqrt{3}} {4\sqrt{3}} = 200$

.green[ 
$$95\% \textrm{CI} = (-130, 470)$$
]


.darkred[**Enormous confidence intervals, because of enormous variability in samples!**]
]


---

## Simple - SWOR

- **SWOR:** Sampling *without* replacement, i.e. design guarantees no individual is counted more than once. 


$$SE(\widehat{D_{swor}}) = SE(\widehat{D_{swr}}) \times \sqrt{1-{a / A}}$$

The larger the proportion sampled (*coverage*) - the smaller the **sampling error.**

--

## Ratio (SWR/SWOR)

**Ratio**: unequal sample frames .blue[(e.g. both hula hoops and meter squares)].


- $\widehat{D} = \sum n_i / \sum y_i$ (same as before)

- Standard errors: more complicated ... see formulae. 

---

## Take-aways

- Is it **very important** to quantify uncertainty!   But also, can be **hard** (and **disheartening**).

--

- Larger samples & higher coverage $\to$ smaller errors $\to$ narrower confidence intervals $\to$ more precision. 

--

- The error estimates take into account **sample randomness**, but also **heterogeneity**.  The more **heterogeneous** the distribution the larger the errors. 

--

- Which is why ... .green[you take that heterogeneity into in your **estimates!**]
 

---

# One more formula ... for combining estimates

.pull-left[
If you have multiple sub-count estimates (e.g. one for each of $r$ sub-region): 

.darkred[

-  $\widehat{N_1}, \widehat{N_2},  ..., \widehat{N_r},$ 

]

and each estimate has a standard error: 

.darkred[ 

- $SE(\widehat{N_1}), SE(\widehat{N_2}), ..., SE(\widehat{N_r})$ 

]

Then ... ]

--

.pull-right[

... the **total** estimate will be: 

.darkgreen[

$$\widehat{N} = \sum_{i = 1}^r \widehat{N_i}$$

]

and the standard error will be:

.darkgreen[

$$SE(\widehat{N}) = \sqrt{\sum_{i = 1}^r SE(\widehat{N_i})^2}$$

]

]

--

.center[Will the estimate be more precise?   **You get to test this out in the field!**]
